{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMJM+VVphI3NcEbw7lBR/2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noswad/Python/blob/master/RCMAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "rGr299HW7quD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import joblib\n",
        "\n",
        "class RCMAB:\n",
        "    def __init__(self, context_dim, action_dim, hidden_dim, kappa):\n",
        "        self.context_dim = context_dim  # 上下文的維度（特徵數量）\n",
        "        self.action_dim = action_dim    # 動作的維度（可選動作數量）\n",
        "        self.kappa = kappa              # 用於控制貝葉斯回歸中獎勵的權重\n",
        "\n",
        "        # 初始化神經網絡\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(context_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        # 初始化貝葉斯回歸參數\n",
        "        self.mu = np.zeros(action_dim)  # 貝葉斯回歸的均值參數\n",
        "        self.sigma = np.ones(action_dim)  # 貝葉斯回歸的標準差參數\n",
        "        self.memory = []  # 用於存儲訓練資料\n",
        "\n",
        "        # 初始化優化器\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n",
        "\n",
        "    def predict(self, context):\n",
        "        \"\"\" 預測給定上下文的獎勵 \"\"\"\n",
        "        context_tensor = torch.tensor(context, dtype=torch.float32)\n",
        "        with torch.no_grad():  # 在不計算梯度的情況下進行前向傳播\n",
        "            return self.model(context_tensor).numpy()\n",
        "\n",
        "    def sample_parameters(self):\n",
        "        \"\"\" 抽樣貝葉斯回歸參數 \"\"\"\n",
        "        return np.random.normal(self.mu, self.sigma)\n",
        "\n",
        "    def select_action(self, context):\n",
        "        \"\"\" 根據上下文選擇最佳動作 \"\"\"\n",
        "        predicted_rewards = self.predict(context)\n",
        "        sampled_parameters = self.sample_parameters()\n",
        "        mv_values = self.kappa * predicted_rewards - np.square(sampled_parameters)\n",
        "        return np.argmax(mv_values)\n",
        "\n",
        "    def update(self, context, action, reward):\n",
        "        \"\"\" 更新模型參數 \"\"\"\n",
        "        self.memory.append((context, action, reward))  # 添加訓練資料到記憶體\n",
        "\n",
        "        # 更新神經網絡\n",
        "        context_tensor = torch.tensor(context, dtype=torch.float32)\n",
        "        reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(context_tensor)[action]\n",
        "        loss = (output - reward_tensor).pow(2).mean()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def train(self, contexts, actions, rewards, epochs=100):\n",
        "        \"\"\" 訓練模型 \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            for context, action, reward in zip(contexts, actions, rewards):\n",
        "                self.update(context, action, reward)\n",
        "\n",
        "    def evaluate(self, contexts, actions, rewards):\n",
        "        \"\"\" 評估模型 \"\"\"\n",
        "        total_loss = 0\n",
        "        for context, action, reward in zip(contexts, actions, rewards):\n",
        "            context_tensor = torch.tensor(context, dtype=torch.float32)\n",
        "            reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
        "            with torch.no_grad():\n",
        "                output = self.model(context_tensor)[action]\n",
        "                loss = (output - reward_tensor).pow(2).mean().item()\n",
        "                total_loss += loss\n",
        "        return total_loss / len(contexts)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\" 保存模型 \"\"\"\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        \"\"\" 加載模型 \"\"\"\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        self.model.eval()  # 設置為評估模式\n",
        "\n",
        "# 定義脈絡、動作和獎勵函數\n",
        "def get_current_context():\n",
        "    # 回傳使用者的特徵向量，例如年齡、性別等\n",
        "    return {\"age\": 25, \"gender\": \"male\"}\n",
        "\n",
        "# 創建數據集\n",
        "context_dim = 5\n",
        "action_dim = 3\n",
        "data_size = 100\n",
        "contexts = np.random.rand(data_size, context_dim)\n",
        "actions = np.random.randint(0, action_dim, size=data_size)\n",
        "rewards = np.random.rand(data_size)\n",
        "\n",
        "# 分割訓練和測試數據\n",
        "X_train, X_test, y_train, y_test = train_test_split(contexts, rewards, test_size=0.2, random_state=42)\n",
        "\n",
        "# 使用 Optuna 進行貝葉斯優化\n",
        "def objective(trial):\n",
        "    hidden_dim = trial.suggest_int('hidden_dim', 5, 50)\n",
        "    kappa = trial.suggest_float('kappa', 0.1, 5.0)\n",
        "\n",
        "    model = RCMAB(context_dim, action_dim, hidden_dim, kappa)\n",
        "    model.train(X_train, actions[:len(X_train)], y_train, epochs=100)\n",
        "    loss = model.evaluate(X_test, actions[len(X_train):], y_test)\n",
        "\n",
        "    return loss\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# 最佳參數\n",
        "best_hidden_dim = study.best_params['hidden_dim']\n",
        "best_kappa = study.best_params['kappa']\n",
        "print(f\"Best hidden_dim: {best_hidden_dim}, Best kappa: {best_kappa}\")\n",
        "\n",
        "# 使用最佳參數重新訓練模型\n",
        "best_model = RCMAB(context_dim, action_dim, best_hidden_dim, best_kappa)\n",
        "best_model.train(X_train, actions[:len(X_train)], y_train, epochs=100)\n",
        "loss = best_model.evaluate(X_test, actions[len(X_train):], y_test)\n",
        "print(f\"Evaluation loss: {loss}\")\n",
        "\n",
        "# 保存模型\n",
        "model_path = 'best_rcmab_model.pth'\n",
        "best_model.save_model(model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# 加載並評估模型\n",
        "loaded_model = RCMAB(context_dim, action_dim, best_hidden_dim, best_kappa)\n",
        "loaded_model.load_model(model_path)\n",
        "loaded_loss = loaded_model.evaluate(X_test, actions[len(X_train):], y_test)\n",
        "print(f\"Loaded model evaluation loss: {loaded_loss}\")"
      ],
      "metadata": {
        "id": "icic358y8GVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}